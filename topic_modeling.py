# -*- coding: utf-8 -*-
"""TopicModeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AGwTyd8huBvOqoAJ7FdQJsgyMDYooAPC

# **Topic modeling**


Topic modeling is a frequently used approach that leverages unsupervised machine learning to discover hidden semantic patterns portrayed by a text corpus and automatically identify topics that exist inside it.

---

# **Importing Libraries**
"""

import pandas as pd
import numpy as np
import string
from matplotlib import pyplot as plt
import re
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import Counter
from nltk.stem import PorterStemmer
from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from wordcloud import WordCloud
import seaborn as sns
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
from sklearn.metrics import silhouette_score
import pickle

nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger_eng')

"""# **Reading Data**"""

data_frame = pd.read_csv("dataset/articles1.csv")
data_frame.head()

"""# **Data Cleansing**

**Deal with missing values**
"""

# Calculate percentage of nulls for each column
null_percent = (data_frame.isnull().sum() / len(data_frame)) * 100

# Drop the first column if you want to exclude it from the plot
null_percent_plot = null_percent.drop(index=data_frame.columns[0])

# Plotting
plt.figure(figsize=(25, 10))
plt.bar(null_percent_plot.index, null_percent_plot.values, color='green')
plt.xlabel('Columns')
plt.ylabel('Percentage of Null Values')
plt.title('Null Value Percentage by Column')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Print full null percentage summary
print("percentage of nulls for each column")
print(null_percent)

# Remove url column because it's empty(100% null values)
data_frame.drop(columns=['url'], inplace=True)
data_frame.isnull().sum() / len(data_frame) * 100

# Get percentage of number of rows contain missing values
mask = data_frame.isnull().any(axis=1)
rows_with_missing_values = mask.sum() / len(data_frame)*100
print("Number of rows contain null values =", rows_with_missing_values, "%")

# Drop rows contain null values
data_frame.dropna(inplace=True)
data_frame.isnull().sum() / len(data_frame) * 100

data_frame.dtypes

"""**Changing dtypes**"""

# Convert all object-type columns to string type
object_columns = data_frame.select_dtypes(include='object').columns
data_frame[object_columns] = data_frame[object_columns].astype('string')

# Confirm the changes
print(data_frame.dtypes)

"""**Dealing with Datetime format**"""

# Ensure 'date' column is in datetime format
data_frame['date'] = pd.to_datetime(data_frame['date'], errors='coerce')

# Extract the day from the 'date' column
data_frame['day'] = data_frame['date'].dt.day

# Option 1: Replace 'date' column with 'day' column
data_frame.drop(columns=['date'], inplace=True)

"""**Drop un nesessary coulmns**"""

data_frame.drop(columns=['id'], inplace=True)

"""**Casting**"""

data_frame['year'] = data_frame['year'].astype('Int64')
data_frame['month'] = data_frame['month'].astype('Int64')

data_frame.dtypes

data_frame.head()





"""# Text Preprocessing NLP Pipeline
## 1- Convert to lowercase
"""

columns = ["title", "publication", "author", "content"]

data_frame[columns] = data_frame[columns].apply(lambda x: x.str.lower())

"""## 2- Remove HTML Tags




"""

def remove_HTML_tags(text):
    return re.sub(r'<.*?>', "", text)

columns = ["title", "author", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_HTML_tags(x))

"""## 3- Remove URLs




"""

def remove_URLs(text):
    return re.sub(r'https?://\S+www\.\S+', "", text)

columns = ["title", "author", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_URLs(x))

"""## 4- Remove unnecessary words




"""

# remove unnecessary words
# remove publication from title
publication_unique_values = data_frame["publication"].unique()
for publication in publication_unique_values:
    data_frame["title"] = data_frame['title'].str.replace("." + publication + "$", "", regex=True)

"""## 5- Apply Tokenization
### a- Apply sentence tokenization
"""

def sentence_tokenizer(text):
    return sent_tokenize(text)


columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: sentence_tokenizer(x))

data_frame[['title', 'content']]

"""### b- Apply word tokenization"""

def word_tokenizer(sentences):
    tokenized_words = []
    for x in sentences:
        tokenized_words = tokenized_words + word_tokenize(x)
    return tokenized_words


columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: word_tokenizer(x))

data_frame.head()

"""## 6- Remove Stop Words"""

STOPWORDS = set(stopwords.words("english"))
def remove_stop_words(text):
    return [word for word in text if word not in STOPWORDS]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_stop_words(x))

data_frame.head()

"""## 7- Remove Punctuations"""

punctuation = string.punctuation
def remove_punctuations(text):
    return [''.join(char for char in word if char not in punctuation) for word in text]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_punctuations(x))

data_frame.head(1)

"""## 8- Remove Special characters"""

def remove_special_characters_from_list(text_list):
    return [re.sub(r'[^a-zA-Z\s]', '', word) for word in text_list]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(remove_special_characters_from_list)

data_frame.head(1)

"""## 9- Remove Empty Indexes"""

# remove empty indexes
def remove_empty_indexes(text):
    return [item for item in text if item]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_empty_indexes(x))

data_frame.head(1)

"""## 10- Remove (words) that are only one character long"""

# Remove items (words) that are only one character long
def remove_one_character(text):
    return [item for item in text if len(item) > 1]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(remove_one_character)

data_frame.head(1)

"""## 11- Remove frequently words"""

word_counter_title = Counter()
word_counter_content = Counter()
def get_most_frequently_words(column_name, word_counter):
    for text in data_frame[column_name]:
        for word in text:
            word_counter[word] += 1
    return word_counter

# get the most common frequently words in [title] more than 1000
word_frequency_title = get_most_frequently_words("title", word_counter_title).most_common(20)

# get the most common frequently words in [content]
word_frequency_content = get_most_frequently_words("content", word_counter_content).most_common(20)

text = data_frame['title'].values

wordcloud = WordCloud().generate(str(text))

plt.imshow(wordcloud)
plt.axis("off")
plt.show()

text = data_frame['content'].values

wordcloud = WordCloud().generate(str(text))

plt.imshow(wordcloud)
plt.axis("off")
plt.show()

FREQUENT_WORDS_TITLE = set(word for (word, word_count) in get_most_frequently_words("title", word_counter_title).most_common(20))
FREQUENT_WORDS_CONTENT = set(word for (word, word_count) in get_most_frequently_words("content", word_counter_content).most_common(20))

def remove_frequent_words(string_text, frequent_words):
    return [word for word in string_text if word not in frequent_words]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_frequent_words(x, FREQUENT_WORDS_TITLE))

data_frame.head(1)

"""## 12- Remove Rare words"""

RARE_WORDS_TITLE = set(word for (word, word_count) in get_most_frequently_words("title", word_counter_title).most_common()[:-20:-1])
RARE_WORDS_CONTENT = set(word for (word, word_count) in get_most_frequently_words("content", word_counter_content).most_common()[:-20:-1])

def remove_rare_words(string_text, rare_words):
    return [word for word in string_text if word not in rare_words]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_rare_words(x, RARE_WORDS_TITLE))

data_frame.head(1)

"""## 13- Stemming"""

porter_stemmer_title = PorterStemmer()
porter_stemmer_content = PorterStemmer()
def stemming_words(text, porter_stemmer):
    return [porter_stemmer.stem(word) for word in text]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: stemming_words(x, porter_stemmer_title))

data_frame.head()

"""## POS and Lemmitization"""

lemmatizer_title = WordNetLemmatizer()
lemmatizer_content = WordNetLemmatizer()
wordnet_map = {"N": wordnet.NOUN, "V": wordnet.VERB, "J": wordnet.ADJ, "R": wordnet.ADV}

def lemmatize_words(text, lemmatizer):
    # apply POS tagging
    pos_text = pos_tag(text)
    return [lemmatizer.lemmatize(word, wordnet_map.get(pos[0],  wordnet.NOUN)) for word, pos in pos_text]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: lemmatize_words(x, lemmatizer_title))



"""# Data Preprocessing
## 1- Dealing with categorical data (Nominal Data)
### a- Preprocess [ publication ]
"""

# get number of unique values in publication column
publication_unique_values = data_frame["publication"].unique()
print("Publication unique values = ", len(publication_unique_values))

# apply one hot encoding in publication column
data_frame = pd.get_dummies(data=data_frame, columns=["publication"])
data_frame.dtypes

"""### b- Preprocess [ author ]"""

# clean author
def clean_name(name):
    name = name.strip() # remove spaces after and before string
    name = " ".join(name.split()) # remove extra spaces between words
    name = re.sub(r'^.\s|\s.$', "", name) # remove single character in beginning and end of string

    if len(name) > 1:
        return name

author_unique_values = Counter()
def get_unique_names(author_list):
    for name in author_list:
        if name!=None and len(name) > 1:
            author_unique_values[name] += 1 # get unique values

def preprocess_author(text):
    text = re.sub(r'[a-z]{1}\.|\(.*\)', "", text) # remove single character in beginning and end of string
    author_list = re.split('with|and|,|&', text)
    author_list = [clean_name(name) for name in author_list]

    # check if there is a None value
    if None in author_list:
        author_list.remove(None)

    get_unique_names(author_list)
    return author_list

data_frame["author"] = data_frame["author"].apply(lambda x: preprocess_author(x))
data_frame["author"]

# get number of empty cells in author
result = data_frame['author'].apply(lambda x: isinstance(x, list) and len(x) == 0)
count = result.sum()

# Print the count
print("Number of empty cells is:",count)

# make empty cell = null
data_frame["author"] = data_frame["author"].apply(lambda y: np.nan if len(y)==0 else y)

# remove rows with NaN values
data_frame.dropna(inplace=True)

# get number of empty cells in author
result = data_frame['author'].apply(lambda x: isinstance(x, list) and len(x) == 0)
count = result.sum()

# Print the count
print("Number of empty cells is:",count)

#filtered author column to make each row contain only one author
def filtered_author(author_list):
    frequency = []
    for author in author_list:
        frequency.append(author_unique_values[author])

    # Find the minimum number in the list
    min_number_index = frequency.index(min(frequency))
    return author_list[min_number_index]

data_frame["author"] = data_frame["author"].apply(lambda x: filtered_author(x))

data_frame["author"]

# apply mapping in author
mapping_author = {}
for index, author in enumerate(list(author_unique_values.keys())):
    mapping_author[author] = index

data_frame = data_frame.replace ({
    "author": mapping_author
})

print("Number of unique values =", len(author_unique_values))
data_frame["author"]

"""# Text Representation and Topic Modeling
## 1. TF-IDF Vectorization
"""

# Combine preprocessed content into single strings for TF-IDF
data_frame['content_processed'] = data_frame['content'].apply(lambda x: ' '.join(x))

# Create TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limit features to manage computational complexity
tfidf_matrix = tfidf_vectorizer.fit_transform(data_frame['content_processed'])

# Get feature names (words)
feature_names = tfidf_vectorizer.get_feature_names_out()

"""## 2. Hyperparameter Optimization"""

# Find optimal number of clusters using elbow method and silhouette scores
max_clusters = 15
inertias = []
silhouette_scores = []

for k in range(2, max_clusters + 1):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(tfidf_matrix)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(tfidf_matrix, kmeans.labels_))

# Plot elbow curve
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(range(2, max_clusters + 1), inertias, marker='o')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method')

# Plot silhouette scores
plt.subplot(1, 2, 2)
plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis')
plt.tight_layout()
plt.show()

# Find optimal k
optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2
print(f"\nOptimal number of clusters based on silhouette score: {optimal_k}")

"""## 3. Topic Modeling with Optimal K-Means"""

# Function to print top words per cluster
def print_top_words_per_cluster(cluster_centers, feature_names, n_words=10):
    top_words = {}
    for i, center in enumerate(cluster_centers):
        # Get indices of top n words
        top_indices = center.argsort()[-n_words:][::-1]
        top_words[i] = [feature_names[idx] for idx in top_indices]
    return top_words

# Initialize and fit KMeans
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
cluster_labels = kmeans.fit_predict(tfidf_matrix)

# Add cluster labels to dataframe
data_frame['cluster'] = cluster_labels

# Get top words for each cluster
top_words_per_cluster = print_top_words_per_cluster(kmeans.cluster_centers_, feature_names)

"""## 4. Visualizations"""

# Plot cluster sizes
plt.figure(figsize=(10, 6))
cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()
plt.bar(range(optimal_k), cluster_sizes)
plt.title('Distribution of Documents Across Clusters')
plt.xlabel('Cluster')
plt.ylabel('Number of Documents')
plt.show()

# Generate word clouds for each cluster
for cluster_id in range(optimal_k):
    cluster_docs = data_frame[data_frame['cluster'] == cluster_id]['content_processed']
    if len(cluster_docs) > 0:
        text = ' '.join(cluster_docs)
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'Word Cloud for Cluster {cluster_id}')
        plt.show()
        # save image
        wordcloud.to_file(f'images/wordcloud_cluster_{cluster_id}.png')
        
        print(f"\nTop words in Cluster {cluster_id}:")
        print(", ".join(top_words_per_cluster[cluster_id]))

"""## 5. Model Evaluation"""

# Calculate silhouette score for clustering evaluation
silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)
print(f"\nSilhouette Score: {silhouette_avg}")

# Split data for model validation
train_matrix, test_matrix = train_test_split(tfidf_matrix, test_size=0.2, random_state=42)

# Fit on training data
train_labels = kmeans.fit_predict(train_matrix)

# Predict on test data
test_labels = kmeans.predict(test_matrix)

# Calculate silhouette score on test data
test_silhouette = silhouette_score(test_matrix, test_labels)
print(f"Test Set Silhouette Score: {test_silhouette}")


# Save model
with open('models/model.pkl', 'wb') as model_file:
    pickle.dump(kmeans, model_file)
# Save vectorizer
with open('models/tfidf_vectorizer.pkl', 'wb') as vectorizer_file:
    pickle.dump(tfidf_vectorizer, vectorizer_file)