# -*- coding: utf-8 -*-
"""TopicModeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AGwTyd8huBvOqoAJ7FdQJsgyMDYooAPC

# **Topic modeling**


Topic modeling is a frequently used approach that leverages unsupervised machine learning to discover hidden semantic patterns portrayed by a text corpus and automatically identify topics that exist inside it.

---

# **Importing Libraries**
"""

import pandas as pd
import numpy as np
import string
from matplotlib import pyplot as plt
import re
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import Counter
from nltk.stem import PorterStemmer
from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from wordcloud import WordCloud
import seaborn as sns
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger_eng')

"""# **Reading Data**"""

data_frame = pd.read_csv("//content/drive/MyDrive/NlP project/articles1.csv")
data_frame.head()

"""# **Data Cleansing**

**Deal with missing values**
"""

# Calculate percentage of nulls for each column
null_percent = (data_frame.isnull().sum() / len(data_frame)) * 100

# Drop the first column if you want to exclude it from the plot
null_percent_plot = null_percent.drop(index=data_frame.columns[0])

# Plotting
plt.figure(figsize=(25, 10))
plt.bar(null_percent_plot.index, null_percent_plot.values, color='green')
plt.xlabel('Columns')
plt.ylabel('Percentage of Null Values')
plt.title('Null Value Percentage by Column')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Print full null percentage summary
print("percentage of nulls for each column")
print(null_percent)

# Remove url column because it's empty(100% null values)
data_frame.drop(columns=['url'], inplace=True)
data_frame.isnull().sum() / len(data_frame) * 100

# Get percentage of number of rows contain missing values
mask = data_frame.isnull().any(axis=1)
rows_with_missing_values = mask.sum() / len(data_frame)*100
print("Number of rows contain null values =", rows_with_missing_values, "%")

# Drop rows contain null values
data_frame.dropna(inplace=True)
data_frame.isnull().sum() / len(data_frame) * 100

data_frame.dtypes

"""**Changing dtypes**"""

# Convert all object-type columns to string type
object_columns = data_frame.select_dtypes(include='object').columns
data_frame[object_columns] = data_frame[object_columns].astype('string')

# Confirm the changes
print(data_frame.dtypes)

"""**Dealing with Datetime format**"""

# Ensure 'date' column is in datetime format
data_frame['date'] = pd.to_datetime(data_frame['date'], errors='coerce')

# Extract the day from the 'date' column
data_frame['day'] = data_frame['date'].dt.day

# Option 1: Replace 'date' column with 'day' column
data_frame.drop(columns=['date'], inplace=True)

"""**Drop un nesessary coulmns**"""

data_frame.drop(columns=['id'], inplace=True)

"""**Casting**"""

data_frame['year'] = data_frame['year'].astype('Int64')
data_frame['month'] = data_frame['month'].astype('Int64')

data_frame.dtypes

data_frame.head()





"""# Text Preprocessing NLP Pipeline
## 1- Convert to lowercase
"""

columns = ["title", "publication", "author", "content"]

data_frame[columns] = data_frame[columns].apply(lambda x: x.str.lower())

"""## 2- Remove HTML Tags




"""

def remove_HTML_tags(text):
    return re.sub(r'<.*?>', "", text)

columns = ["title", "author", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_HTML_tags(x))

"""## 3- Remove URLs




"""

def remove_URLs(text):
    return re.sub(r'https?://\S+www\.\S+', "", text)

columns = ["title", "author", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_URLs(x))

"""## 4- Remove unnecessary words




"""

# remove unnecessary words
# remove publication from title
publication_unique_values = data_frame["publication"].unique()
for publication in publication_unique_values:
    data_frame["title"] = data_frame['title'].str.replace("." + publication + "$", "", regex=True)

"""## 5- Apply Tokenization
### a- Apply sentence tokenization
"""

def sentence_tokenizer(text):
    return sent_tokenize(text)


columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: sentence_tokenizer(x))

data_frame[['title', 'content']]

"""### b- Apply word tokenization"""

def word_tokenizer(sentences):
    tokenized_words = []
    for x in sentences:
        tokenized_words = tokenized_words + word_tokenize(x)
    return tokenized_words


columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: word_tokenizer(x))

data_frame.head()

"""## 6- Remove Stop Words"""

STOPWORDS = set(stopwords.words("english"))
def remove_stop_words(text):
    return [word for word in text if word not in STOPWORDS]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_stop_words(x))

data_frame.head()

"""## 7- Remove Punctuations"""

punctuation = string.punctuation
def remove_punctuations(text):
    return [''.join(char for char in word if char not in punctuation) for word in text]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_punctuations(x))

data_frame.head(1)

"""## 8- Remove Special characters"""

def remove_special_characters_from_list(text_list):
    return [re.sub(r'[^a-zA-Z\s]', '', word) for word in text_list]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(remove_special_characters_from_list)

data_frame.head(1)

"""## 9- Remove Empty Indexes"""

# remove empty indexes
def remove_empty_indexes(text):
    return [item for item in text if item]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_empty_indexes(x))

data_frame.head(1)

"""## 10- Remove (words) that are only one character long"""

# Remove items (words) that are only one character long
def remove_one_character(text):
    return [item for item in text if len(item) > 1]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(remove_one_character)

data_frame.head(1)

"""## 11- Remove frequently words"""

word_counter_title = Counter()
word_counter_content = Counter()
def get_most_frequently_words(column_name, word_counter):
    for text in data_frame[column_name]:
        for word in text:
            word_counter[word] += 1
    return word_counter

# get the most common frequently words in [title] more than 1000
word_frequency_title = get_most_frequently_words("title", word_counter_title).most_common(20)

# get the most common frequently words in [content]
word_frequency_content = get_most_frequently_words("content", word_counter_content).most_common(20)

text = data_frame['title'].values

wordcloud = WordCloud().generate(str(text))

plt.imshow(wordcloud)
plt.axis("off")
plt.show()

text = data_frame['content'].values

wordcloud = WordCloud().generate(str(text))

plt.imshow(wordcloud)
plt.axis("off")
plt.show()

FREQUENT_WORDS_TITLE = set(word for (word, word_count) in get_most_frequently_words("title", word_counter_title).most_common(20))
FREQUENT_WORDS_CONTENT = set(word for (word, word_count) in get_most_frequently_words("content", word_counter_content).most_common(20))

def remove_frequent_words(string_text, frequent_words):
    return [word for word in string_text if word not in frequent_words]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_frequent_words(x, FREQUENT_WORDS_TITLE))

data_frame.head(1)

"""## 12- Remove Rare words"""

RARE_WORDS_TITLE = set(word for (word, word_count) in get_most_frequently_words("title", word_counter_title).most_common()[:-20:-1])
RARE_WORDS_CONTENT = set(word for (word, word_count) in get_most_frequently_words("content", word_counter_content).most_common()[:-20:-1])

def remove_rare_words(string_text, rare_words):
    return [word for word in string_text if word not in rare_words]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: remove_rare_words(x, RARE_WORDS_TITLE))

data_frame.head(1)

"""## 13- Stemming"""

porter_stemmer_title = PorterStemmer()
porter_stemmer_content = PorterStemmer()
def stemming_words(text, porter_stemmer):
    return [porter_stemmer.stem(word) for word in text]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: stemming_words(x, porter_stemmer_title))

data_frame.head()

"""## POS and Lemmitization"""

lemmatizer_title = WordNetLemmatizer()
lemmatizer_content = WordNetLemmatizer()
wordnet_map = {"N": wordnet.NOUN, "V": wordnet.VERB, "J": wordnet.ADJ, "R": wordnet.ADV}

def lemmatize_words(text, lemmatizer):
    # apply POS tagging
    pos_text = pos_tag(text)
    return [lemmatizer.lemmatize(word, wordnet_map.get(pos[0],  wordnet.NOUN)) for word, pos in pos_text]

columns = ["title", "content"]
for column in columns:
    data_frame[column] = data_frame[column].apply(lambda x: lemmatize_words(x, lemmatizer_title))



"""# Data Preprocessing
## 1- Dealing with categorical data (Nominal Data)
### a- Preprocess [ publication ]
"""

# get number of unique values in publication column
publication_unique_values = data_frame["publication"].unique()
print("Publication unique values = ", len(publication_unique_values))

# apply one hot encoding in publication column
data_frame = pd.get_dummies(data=data_frame, columns=["publication"])
data_frame.dtypes

"""### b- Preprocess [ author ]"""

# clean author
def clean_name(name):
    name = name.strip() # remove spaces after and before string
    name = " ".join(name.split()) # remove extra spaces between words
    name = re.sub(r'^.\s|\s.$', "", name) # remove single character in beginning and end of string

    if len(name) > 1:
        return name

author_unique_values = Counter()
def get_unique_names(author_list):
    for name in author_list:
        if name!=None and len(name) > 1:
            author_unique_values[name] += 1 # get unique values

def preprocess_author(text):
    text = re.sub(r'[a-z]{1}\.|\(.*\)', "", text) # remove single character in beginning and end of string
    author_list = re.split('with|and|,|&', text)
    author_list = [clean_name(name) for name in author_list]

    # check if there is a None value
    if None in author_list:
        author_list.remove(None)

    get_unique_names(author_list)
    return author_list

data_frame["author"] = data_frame["author"].apply(lambda x: preprocess_author(x))
data_frame["author"]

# get number of empty cells in author
result = data_frame['author'].apply(lambda x: isinstance(x, list) and len(x) == 0)
count = result.sum()

# Print the count
print("Number of empty cells is:",count)

# make empty cell = null
data_frame["author"] = data_frame["author"].apply(lambda y: np.nan if len(y)==0 else y)

# remove rows with NaN values
data_frame.dropna(inplace=True)

# get number of empty cells in author
result = data_frame['author'].apply(lambda x: isinstance(x, list) and len(x) == 0)
count = result.sum()

# Print the count
print("Number of empty cells is:",count)

#filtered author column to make each row contain only one author
def filtered_author(author_list):
    frequency = []
    for author in author_list:
        frequency.append(author_unique_values[author])

    # Find the minimum number in the list
    min_number_index = frequency.index(min(frequency))
    return author_list[min_number_index]

data_frame["author"] = data_frame["author"].apply(lambda x: filtered_author(x))

data_frame["author"]

# apply mapping in author
mapping_author = {}
for index, author in enumerate(list(author_unique_values.keys())):
    mapping_author[author] = index

data_frame = data_frame.replace ({
    "author": mapping_author
})

print("Number of unique values =", len(author_unique_values))
data_frame["author"]

